---
output: html_document
format:
  html:
    code-fold: true
    code-summary: "Show the code"
editor_options: 
  chunk_output_type: console
---
# Discrete Marks

## Dependencies

```{r, cache=FALSE}
source("utils.R")
```

## Setup

```{r, cache=FALSE}
#| label: load-data
spe <- readRDS("../data/spe.rds")

#subset the data to only look at sample ID 0.01
sub <- spe[, spe$sample_id == "0.01"]
(pp <- .ppp(sub, marks = "cluster_id"))

#split the multitype point process into several single type processes
#first, set the marks of the point process to be factors
marks(pp) <- factor(marks(pp))
ppls <- split(pp)
```

<!-- If not otherwise indicated, all information was taken from Baddeley et al. - Spatial Point Patterns. -->

```{r, fig.width=12, fig.height=12, cache=FALSE}
#Plot the marks separately 
plot(ppls, main = 'Point Pattern Marks Separated', nrows = 2)
```

## Concepts and Definitions of Point Processes

### Point Process

```{r}

```

### Complete Spatial Randomness

Complete spatial randomness (CSR) is often used as the null model for various point patterns, and is the result of a Poisson process. A completely random process is characterised by two properties, homogeneity and independence, as discussed below.

#### Homogeneity

Homogeneity implies that the expected number of points falling into a given region $B$ is proportional to its area $|B|$ given a proportionality constant $\lambda$. The constant $\lambda$ represents the intensity of the process, i.e., the average number of points in a unit area [1]:

$$
\mathbb{E}[X\cap B] = \lambda |B|.
\label{eq:expected_number_points}
$$

#### Independence

Independence implies that in two (non-overlapping) regions $A$ and $B$, the number of points $n(X\cap A)$ and $n(X\cap B)$ are independent random variables. In other words, the number of points in region $A$ does not affect the number of points in region $B$. In addition, the number of points, $N = n(X\cap B)$, follows a Poisson distribution:

$$
\mathbb{P}[N=k] = e^{-\mu}\frac{\mu^k}{k!}\\
\label{eq:poisson_process}
$$
where $k = \lambda |B|$ [1].

### Inhomogeneous Poisson Process

A Poisson process that is spatially varying in its average density of points is called inhomogeneous. Here, the average density, $\lambda(u)$, sometimes known as the intensity function (see below), is a function of spatial location $u$. In this case, the expected number of points falling into a region $B$, $\mu = n(X\cap B)$, is an integration of the intensity function over the region [1]:

$$
\mu = \int_{B} \lambda(u) du.
\label{eq:expected_number_inhomogeneous}
$$

### Isotropy

A point process is called isotropic, if its statistical properties are invariant to rotations; a CSR process is both stationary and isotropic [1].

### Stationarity

A point process is called stationary if, when viewed through a window $W$, its statistical properties do not depend on the location of the window [1].

#### Correlation stationarity

It is important to note that the metrics designed for inhomogeneous point processes should not be applied to every spatially inhomogeneous point process. They should only be applied to point processes that are ``correlation stationary'', which specifies the metric only depends on the relative position in subpatterns of the point process, i.e., that estimates of the inhomogeneous metric should be similar in different subquadrats of the point pattern [1].

### Local scaling

The inhomogeneous K-function further assumes that while the intensity is spatially-varying, the scale of the interaction remains constant. This is equivalent to the assumption that in small subregions, the process is stationary and isotropic, but the rescaling factor can vary across the total process. In this case the locally-scaled version of the K-function is applicable [1].

We can use a permutation test to test the inhomogeneity assumption. In this scenario, we split the patterns into quadrats and compare the estimatied functions between the quadrats. It should be noted that this test depends on the arbitrary definition of the quadrats [MR: add reference].

```{r, warning=FALSE}
pp_sel <-  subset(pp, marks %in% "OD Mature", drop = TRUE)

odrho <- rhohat(unmark(pp_sel), "x", method="tr")
odlambda <- predict(odrho)

od4 <- quantess(unmark(pp_sel), "x", 2)
od42 <- nestsplit(pp_sel, od4, ny=3)

plot(od42)

od42$inten <- factor(as.integer(od42$f1) <= 1, labels=c("Hi","Lo"))

res.scaled <- studpermu.test(od42, pts ~ inten, summaryfunction=Kscaled,
               minpoints = 10)

res.inhom <- studpermu.test(od42, pts ~ inten, summaryfunction=Kinhom,
               lambda=odlambda, minpoints = 10)

#p-value of the local-scaling test
res.scaled$p.value

#p-value of the inhomogeneity test
res.inhom$p.value
```

[MR: how do we interpret these P-values? Should we describe more?]


Alternatively, we can inspect departures from the hypothesis that points were generated by a Poisson process. We can identify hotspots and coldspots by comparing the standard error of the `relrisk` function, which computes nonparamatric estimates of the relative risk by kernel smoothing, to the theoretical null distribution of points. The relative risk is the ratio of spatially varying probablilities of different types. [MR: can we make this reference a proper (numbered) reference?] [Source](https://idblr.rbind.io/post/pvalues-spatial-segregation/)


#TODO: We should maybe move this to after the intensity part
[MR: agree, this should come right after quadrat counting]

```{r}
# select marks
selection <- c('OD Mature', 'Ependymal', 'Microglia')
pp_sel <-  subset(pp, marks %in% selection, drop = TRUE)

f1 <- pValuesHotspotMarks(pp_sel)

# Plot significant p-values
plot(f1$p, main = "Significant difference\n to Poisson process alpha = 0.05")
```

[MR: can we skip this one below? It represents the diff to Poisson for 3 cell types combined, which seems difficult to interpret]

```{r}
f0 <- pValuesHotspot(pp_sel)

# Plot significant p-values
plot(f0$p, main = "Significant difference\n to Poisson process alpha = 0.05")
```


### Intensity

Intensity is the expected density of points per unit area, as seen above [MR: where was it seen above?]. It can be interpreted as the rate of occurrence or the abundance of events recorded. The intensity represents a first moment property because it is related to the expected number of points [1]. [MR: worth mentioning the lambda function here at all? Just to connect with above.]

#### Estimating Intensity

For a homogeneous point process, the intensity can be estimated in a simplistic way: summing the individual intensities of the marks [1].

```{r, cache=FALSE}
intensityPointProcess <- function(pp,mark) if(mark) intensity(pp) else sum(intensity(pp))

intensityPointProcess(pp, mark = FALSE) %>% round(6)
```
[MR: proposal above for more compact code]

Otherwise, we can compute the intensity for each mark individually.

```{r, cache=FALSE}
intensityPointProcess(pp, mark = TRUE) %>% round(8)
```

[MR: I suggest to put KERNAL ESTIMATION here, then QUADRAT COUNTING below; then the density estimation part is together and then the density analysis part is together.]

#### Quadrat Counting

In quadrat counting, all points falling into a given quadrat are counted. This gives an overview on the characteristics of the point pattern, such as correlation stationarity [1].

```{r, fig.width=12, fig.height=12, cache=FALSE}
Q5 <- quadratcount(pp, nx=8, ny=8)
plot(unmark(pp), main='Unmarked Point Pattern Quadrats')
plot(Q5, col='black', add=TRUE)
```

Under independence assumptions, the quadrat counts can be used for testing homogeneity, i.e., if the  points are distributed evenly across the quadrats [1].

[MR: how do we adjut for the missing cells at the bottom? Or should we just mention it as a potential disadvantage. And, is all cells the best example? Would it be better to show]

```{r, cache=FALSE}
quadratTestPointProcess <- function(pp, mark){
  if(mark==TRUE){
    return(lapply(split(pp), quadrat.test, 5, alternative="regular", method="MonteCarlo"))
  }
  else{
    return(quadrat.test(unmark(pp), 5, alternative="regular", method="MonteCarlo"))
  }
}
quadratTestPointProcess(pp, mark=FALSE)
```

[MR: given that we only call this function once now, does it still make sense to have a wrapper function. Maybe just call quadrat.test directly?]
[MR: should we also interpret this P-value=1 with a sentence?]

#### Kernel Estimation

In kernel estimation, we try to estimate the intensity function $\lambda(u)$ of the point process. There are a wide variety of kernel estimators (see [1]), but a popular choice is the isotropic Gaussian kernel where the standard deviation corresponds to the smoothing bandwidth [1].

```{r, fig.width=8, fig.height=8}
pp_sel <-  subset(pp, marks %in% "Inhibitory", drop = TRUE)
Dens <- density(pp_sel, sigma = 100)
plot(Dens, main = 'Kernel Density (Inhibitory cells)')
```

### Testing for CSR

Whether or not a point process is completely spatially random (CSR) depends on two characteristics: points need to be distributed homogeneously and they have to be independent of each other (see definitions above). There are various ways to test for CSR that are summarised in the wrapper below [1].

[MR: this wrapper below is not very compact (I have some thoughts ..), but I also wonder .. we only ever use clark-evans, so as above we could just call that one and mention there are other variants?]

```{r, cache=FALSE}
#PRE: takes a point process and the indication, which test for CSR should be performed and potentially a covariate
#POST: returns if a point process or its individual point process marks are CSR or not.
#TODO: Change maybe to a switch statment in terms of computing time; Change lapply to mclapply later on
#TODO: There is a conceptual mistake - we pull out marks and test them against a markov simulation. It should rather be to test against all the other cells
testingCSR <- function(
    pp,
    method = c('quadrat','cdf','bermans','clark-evans','hopkins-skellam'),
    mark = FALSE,
    covariate = NULL,
    test = c('ks', 'cvm', 'ad'),
    verbose = FALSE
){
  #perform a quadrattest for individual marks or for the entire pointprocess
  if(method == 'quadrat'){
    if(mark == TRUE){
      test.result <- lapply(split(pp), quadrat.test, 5, method="MonteCarlo")
    }
    else{
      test.result <- quadrat.test(unmark(pp), 5, method="MonteCarlo")
    }
   }
  #perform a cdftest for individual marks or for the entire pointprocess given a covariate
  else if(method == 'cdf' && !is.null(covariate)){
    if(mark == TRUE){
      test.result <- lapply(split(pp), cdf.test, covariate, test=test)
    }
    else{
      test.result <- cdf.test(unmark(pp), covariate, test=test)
    }
  }
  #perform a bermans test for individual marks or for the entire pointprocess
  else if(method == 'bermans' && !is.null(covariate)){
     if(mark == TRUE){
      test.result <- lapply(split(pp), berman.test, covariate, test='Z1')
    }
    else{
      test.result <- berman.test(unmark(pp), covariate, test='Z1')
    } 
  }
  #perform a clark evans test for individual marks or for the entire pointprocess
  else if(method == 'clark-evans'){
     if(mark == TRUE){
      test.result <- lapply(split(pp), clarkevans.test)
    }
    else{
      test.result <- clarkevans.test(unmark(pp))
    } 
  }
  #perform a hopkins-skellam test for individual marks or for the entire pointprocess
  else if(method == 'hopkins-skellam'){
     if(mark == TRUE){
      test.result <- lapply(split(pp), hopskel.test)
    }
    else{
      test.result <- hopskel.test(unmark(pp))
    } 
  }
  #base case of the "switch" statement
  else{
    print("ERROR: non-specified arguments or methods")
    return(NULL)
  }
  
  #summarise the results as a mask of booleans to indicate which structures are 
  #random and which are not
  if(mark==TRUE){
    p.value.mask <- lapply(test.result, function(x) x$p.value>0.05)
  }
  else{
    p.value.mask <- test.result$p.value>0.05
  }
  #return the values of the test calculations, either just the boolean if 
  #CSR or not or the entire test statistics
  if(verbose == TRUE){
    return(test.result)
  }
  else{
    return(p.value.mask)
  }
}
result <- testingCSR(pp,method='clark-evans',mark=TRUE, verbose=TRUE)

testingCSR(ppls$Ependymal, method='clark-evans')
testingCSR(ppls$`OD Mature`, method='clark-evans')
testingCSR(ppls$Microglia, method='clark-evans')
```

## Correlation

Correlation, or more generally covariance, represents a second-order summary statistic and measures dependence between data points.

````{=html}
<!-- 
### Morisita Index

$$
M = m \frac{\sum_jn_j(n_j-1)}{n(n-1)}
$$

where we subdivide the point process into $m$ quadrats. The formula above is called the Morisita index which is the ratio of observed and expected fractions. If the points are independent the Morisita index is close to 1, greater than 1 if they are clustered, and less than 1 if they are regular. This concept is closely linked to the index of dispersion (in fact with a bit of algebra, these two can be transformed into each other)

$$
I = \frac{s^2}{\bar{n}} = \frac{m}{n(m-1)}\sum_{j=1}^m\left(n_j-\frac{n}{m}\right)^2
$$

```{r, cache=FALSE}
miplot(ppls$Ependymal)
miplot(ppls$`OD Mature`)
miplot(ppls$Microglia)
```

### Fry plot

A fryplot is a scatterplot of the vector differences $x_j-x_i$ between all paris of distinct points in the pattern.

```{r, cache=FALSE}
fryplot(ppls$Ependymal)
fryplot(ppls$`OD Mature`)
fryplot(ppls$Microglia)
``` 
-->
````

### Ripley's $K$

#### Empirical Ripley's $K$

In the framework of correlation analysis, we often look at distances $d_{ij} = ||x_i-x_j||$ of all points. It is then natural to look at the summary of these distances, $d_{ij}$, e.g. as a histogram. The histogram of this point process is a difficult statistic [MR: what does it mean to be a 'difficult statistic'? Maybe we can phrase this better.], as it depends on the observation window $W$, thus the histogram can change significantly with a different window. Therefore, we look at the empirical distribution function of the distances $d_{ij}$ that are smaller or equal than a radius $r$ [1]

$$
\hat{H}(r) = \frac{1}{n(n-1)}\sum_{i=1}^n \sum_{j=1, j \neq i}^n I\{d_{ij}\leq r\},
$$

where $I(.)$ is the indicator function (1 if the condition is true, 0 if not). The contribution of each point $x_i$ to the sum above is:

$$
t_i(r) = \sum_{j \neq i} I \{d_{ij}\leq r\},
$$

where $t_i(r)$ is the number of points that fall within a radius $r$ centered at $x_i$. It follows then that [1]:

$$
\hat{H}(r) = \frac{1}{n(n-1)}\sum_{i=1}^n t_i(r) = \frac{1}{n-1} \bar{t}(r).
$$

What we actually measure here is "the average number of r-neighbours of a typical random point". This number is still dependent on the size of the observation window so we can standardise it by the number of points and the window size, $|W|$. We then obtain the empirical Ripley's $K$ function [1]:

$$
\hat{K}(r) = \frac{|W|}{n(n-1)}\sum_{i=1}^n\sum_{j=1 \\j \neq i}^n\{d_{ij}\leq r\}.
$$

The standardisation makes it possible to compare point patterns with different observation windows and with different numbers of points. However, using the empirical $K$ function assumes though that the point process has homogeneous intensity [1], which is often not the case for biological tissue. We will return to this issue below in the ``Correcting for Inhomogeneity'' [MR: add link to section below?]

#### True $K$-function

Instead of the summary of pairwise distances, we are interested in the point process. In order to do so, we have to think about the expected number of $r$-neighbours given a point $X$ at a location $u$ divided by its intensity $\lambda$ [1]. This corresponds to:

$$
K(r) = \frac{1}{\lambda} \mathbb{E} [t(u,r,X)|u \in X],
$$

where

$$
t(u,r,X) = \sum_{j=1}^{n(X)} \mathbb{1} \{0<||u-x_j||\leq r\}.

$$
<!--
This definition of the true $K$ function is only valid if the point process is stationary. For a homogeneous Poisson process we obtain

$$
K_{pois}(r) = \pi r^2
$$

-->

[MR: just thinking .. how much of this math do we really need?]

#### Edge effects and their corrections for spatial metrics

Edge effects describe the phenomenon that entire point process is not observed, but rather only the part within the window $W$. This means the value of various statistics could be biased along the edges [1].

There are many corrections for edge effects that are briefly listed here:

##### Border correction

In border correction the summation of data points is restricted to $x_i$ for which $b(x_i,r)$ is completely in the window $W$.

##### Isotropic correction

We can regard edge effect as a sampling bias. Larger distances (e.g. close to the edges) are less likely to be observed. This can be corrected for.

##### Translation correction

A stationary point process $X$ is invariant to translations. So the entire point process can be shifted by a vector $s$ to be at the position $X+s$.

[MR: do we need sub-sub-subsections above? Since there is only 1-2 sentences in each sub-heading, could we compress this all to a paragraph?]

### The $L$-function

The $K$-function can be ``centered'', which is then called the $L$-function. The $L$-function is a variance-stabilising version of the $K$-function (see spicyR for reference) [MR: do proper citation here?]:

$$
L(r) = \sqrt{\frac{K(r)}{\pi}}.
$$

### Pair Correlation function

We have seen above that the $K$-function is cumulative in nature. That is, the contributions of all distances smaller equal to $r$ are considered. An alternative is to take the derivative of the $K$-function in order to obtain contributions of distances between points equal to $r$ [1], according to:

$$
g(r) = \frac{K'(r)}{2\pi r},
$$

where $g(r)$ is the probability of observing a pair of points of the process separated by a distance $r$ divided by the corresponding probability for a Poisson process [1]. [MR: I removed the quotes here because it got to be too many quotes overall. We may need to rephrase a little bit.]

#### Estimator of the pair correlation function

[MR: just wondering .. is the section important?]

The pair correlation function can be estimated via kernel smoothing. In very large datasets the pair correlation function can be approximated using histogram-based methods [1] according to:

$$
\hat{g}(r) = \frac{|W|}{2 \pi r n (n-1)} \sum_{i=1}^n\sum_{j=1, j \neq i}^n \kappa_h(r-d_{ij})e_{ij}(r),
$$

where $\kappa$ is the smoothing kernel. $\kappa_h(x)$ is a rescaled version of the template kernel $\kappa$

$$
\kappa_h(x) = \frac{1}{h}\kappa\left(\frac{x}{h}\right)
$$

In the above, $\kappa$ can be any probability density over the real line with mean 0. Usually, the Epanechinikov kernel is used as smoothing kernel with half-width $w$ [1].

```{r, error=FALSE, message=FALSE, warning=FALSE, results='hide'}
#| message: false
#| warning: false
#PRE: list of point pattern, corresponding celltypes of interest, functions to evaluate
#POST: result of the metric
metricResBoot <- function(ppls, celltype, fun){
  metric.res <- lohboot(ppls[[celltype]], fun = fun)
  metric.res$type <- celltype
  metric.res
}

#PRE: celltypes, function to calculation and edge correction method
#POST: dataframe of 
metricResBootToDF <- function(celltype_ls, ppls, fun, edgecorr){
  lapply(celltype_ls, function(u) {
    metricResBoot(u, fun = fun, ppls = ppls) %>%
      as.data.frame
  }) %>% bind_rows
}

# [MR: try to write the above a litte more compactly]

#PRE: Celltypes of interest, function to analyse, edge correction to perform
#POST: plot of the metric
plotMetric <- function(celltype_ls, ppls, fun, edgecorr){
  res_df <- metricResBootToDF(celltype_ls, ppls, fun, edgecorr)
  #plot the curve
  ggplot(res_df, aes(x=r, y=.data[[edgecorr]], col= type))+
    geom_line()+
    geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25)+
    ggtitle(paste0(fun, '-function'))+
    geom_line(aes(x=r,y=theo, color = 'Poisson'),linetype = "dashed")+
    ylab(edgecorr) +
    scale_color_manual(name='Point Processes',
                     breaks=c('Ependymal', 'Microglia', 
                              'OD Mature', 'Poisson'),
                     values=c('Ependymal'='red', 
                              'Microglia'='dark green', 
                              'OD Mature'='blue', 'Poisson'='black'))+
    theme_light()
}

celltype_ls <- c("Ependymal", "OD Mature", "Microglia")
p_K <- plotMetric(celltype_ls, ppls, 'Kest', 'iso')
p_L <- plotMetric(celltype_ls, ppls, 'Lest', 'iso')
p_g <- plotMetric(celltype_ls, ppls, 'pcf', 'border')
```

```{r, cache=FALSE, fig.height = 10, fig.width = 7}
p_K/p_L/p_g
```

[MR: should we write some sentences to interpret these plots? Or do we even skip it here because it's likely an inhomogeneous process and not valid?]

[MR: could we make a more compact representation like 1 row x 3 columns and have a common legend?]

### Correcting for Inhomogeneity

#### Inhomogeneous $K$-function

In the case that a spatial pattern is known or suspected to be inhomogeneous, we have to take this into account in the analysis. Biological point patterns display inhomogeneity very often, therefore this analysis is preferred over the homogeneous alternatives [MR: this suggest almost that we should skip the homogeneous statistics above, no?]. Inhomogeneous alternatives can be calculated via:

$$
K_{inhom}(r) = \mathbb{E} \left[\sum_{x_j \in X} \frac{1}{\lambda(x_j)}\mathbb{1}\{0<||u-x_j||\leq r\}|u \in X\right].
$$

This theoretical quantity can be approximated with estimators such as:

$$
\hat{K}_{inhom}(r) = \frac{1}{D^p|W|}\sum_i\sum_{j \neq i} \frac{\mathbb{1}\{||u-x_j||\leq r\}}{\hat{\lambda}(x_j)\hat{\lambda}(x_i)}e(x_j,x_i;r),
$$

where $e(u,v;r)$ is an edge correction weight, $\hat{\lambda}(u)$ is an estimator of the intensity of $u$ and $D$ is the following [1]: 

$$
D = \frac{1}{|W|}\sum_i \frac{1}{\hat{\lambda}(x_i)}
$$

[MR: just wondering how important all these formulas are? Do we want a full mathematical description, or does the mathematical description still help us to unpack what is needed for the common situation of inhomogeneous point patterns?]

```{r, error=FALSE, message=FALSE, warning=FALSE, results='hide'}
p_K <- plotMetric(celltype_ls, ppls, 'Kinhom', 'iso')

p_L <- plotMetric(celltype_ls, ppls, 'Linhom', 'iso')

p_g <- plotMetric(celltype_ls, ppls, 'pcfinhom', 'border')
```

```{r, cache=FALSE, fig.height = 10, fig.width = 7}
p_K/p_L/p_g
```

[MR: what if we plotted 1x3 w/ common legend?]

The inhomogeneous $K$-function tells us that the microglia cells follow close to a Poisson process (dashed line) closely and can therefore be assumed to be randomly distributed and not clustered. Ependymal cells show a high degree of clustering at a low radius $r$. OD mature cells exhibit a medium level of clustering.

The $L$-function is a variance stabilised (source spicyR) [MR: proper citation] version of the $K$-function [MR: also, we've already described the L-function above, so we probably don't need to repeat it here]. Thus, the information is complementary to the above. As with the $K$-function, the microglia cells are along the dashed Poisson line, indicating no clustering; ependymal cells are highly clustered at low values of $r$, whereas OD mature show intermediate clustering.

The pair correlation function is the derivative of the $K$-function. Therefore, it is not a sum of the points in the circle with radius $r$ but rather the individual points on the radius $r + h$ where $h$ is very small. The pcf plot gives similar information as before: microglia cells are around the dashed Poisson line. OD Mature cells show a rather broad range of correlations between $r \in [20,100]$. Ependymal cells have a very strong correlation at $\sim r = 25$.

We should further note that the inhomogeneity correction assumes that the process is correlation stationary, meaning that the summary statistics are the same in each quadrat. This is clearly violated at least for Ependymal cells and OD mature cells [MR: how do we know this is violated? Can we test somehow also for this departure?]. Therefore, the question remains whether acounting for one issue (homogeneity) via a correction that assumes correlation stationarity does not just exchange one problem for another. Nonetheless, these measures do give some quantitative summaries of clustering .. [MR: should we add a sentence like this? Otherwise, it does question whether it is valid to do any of this.]

### Local Scaling

#### Locally-scaled $K$-function

In the inhomogeneous $K$-function approach above, we assume that the local scale of the point process is not changed. However, the intensity can vary spatially [MR: Here, I'm struggling to know the difference b/w scale and intensity .. did we clearly specify this]. In a biological sample, this assumption is easily violated,  e.g. when a gradient of cells that increases from one side to another. Therefore, we can assume that the process is subdivided in small regions. In these small regions, the point process is a scaled version of a template process. This template process needs to be both stationary and isotropic. For two locations $u$ and $v$ we would then assume that:

$$
g(u,v) = g_1 \left(\frac{||u-v||}{s}\right).
$$

In this example, $g_1$ is the pair correlation function of the template process and $s$ a scaling factor [1].

#### Locally-scaled pair-correlation function

would work by taking the derivative of the locally scaled $K$-function [MR: this is not written as a proper sentence. And as it's super short, do we need a subsection for it?]

#### Locally-scaled $L$-function

Since the $L$-function is simply a transformation of the $K$-function, the same local scaling can apply to the $L$-function.

```{r, error=FALSE, message=FALSE, warning=FALSE, results='hide'}
### need to redefine the metric function, because bootstrap is not available for locally scaled functions ###
#PRE: list of point pattern, corresponding celltypes of interest, functions to evaluate
#POST: result of the metric
metricRes <- function(ppls, celltype, fun){
  metric.res <- do.call(fun, args = list(X=ppls[[celltype]]))
  metric.res$type <- celltype
  return(metric.res)
}
# [MR: can we write this such that it works for all functions? Like have a super wrapper?]

#PRE: celltypes, function to calculation and edge correction method
#POST: dataframe of 
metricResToDF <- function(celltype_ls, ppls, fun, edgecorr){
  res_ls <- lapply(celltype_ls, metricRes, fun = fun, ppls = ppls)
  #stick all values into a dataframe
  res_df <- c()
  for(i in 1:length(celltype_ls))res_df <- rbind(res_df, res_ls[[i]])
  return(res_df)
}
# [MR: this also doesn't look to different to the one above, which makes me think we should write it also as a super wrapper?]

### need to redefine the plotting function, because bootstrap is not available for locally scaled functions ###
#PRE: Celltypes of interest, function to analyse, edge correction to perform
#POST: plot of the metric
plotScaledMetric <- function(celltype_ls, ppls, fun, edgecorr){
  res_df <- metricResToDF(celltype_ls, ppls, fun, edgecorr)
  #plot the curve
  p <- ggplot(res_df, aes(x=r, y=res_df[[edgecorr]], col= type))+
    geom_line(linewidth=1)+
    ggtitle(paste0(fun, '-function'))+
    geom_line(aes(x=r,y=theo, color = 'Poisson'),linetype = "dashed", linewidth=1)+
    ylab(edgecorr) +
    scale_color_manual(name='Point Processes',
                     breaks=c('Ependymal', 'Microglia', 'OD Mature', 'Poisson'),
                     values=c('Ependymal'='red', 'Microglia'='dark green', 'OD Mature'='blue', 'Poisson'='black'))+
    theme_light()
  return(list(p = p, res_df = res_df))
}

# [MR: again, stark similarity to those above. Should rewrite.]


p_K <- plotScaledMetric(celltype_ls, ppls, 'Kscaled', 'iso')$p
p_L <- plotScaledMetric(celltype_ls, ppls, 'Lscaled', 'iso')$p
```

```{r, cache=FALSE, fig.height = 10, fig.width = 7}
p_K/p_L
```

[MR: as above, should we plot these in 1 plot (1 row, 2 columns, 1 legend)]


```{r, eval=FALSE, include=FALSE}
plotScaledMetric <- function(celltype_ls, ppls, fun, edgecorr){
  res_df <- metricResToDF(celltype_ls, ppls, fun, edgecorr)
  #plot the curve
  p <- ggplot(res_df, aes(x=r, y=.data[[edgecorr]], col= type))+
    geom_line(linewidth=1)+
    ggtitle(paste0(fun, '-function'))+
    geom_line(aes(x=r,y=theo, color = 'Poisson'),linetype = "dashed", linewidth=1)+
    ylab(edgecorr) +
    # scale_color_manual(name='Point Processes',
    #                  breaks=c('Ependymal', 'Microglia', 'OD Mature', 'Poisson'),
    #                  values=c('Ependymal'='red', 'Microglia'='dark green', 'OD Mature'='blue', 'Poisson'='black'))+
    theme_light()
  return(list(p = p, res_df = res_df))
}

functional.pca.pp <- function(res_df){
  df <- res_df
  fd_obj <- fdapace::MakeFPCAInputs(IDs = res_df$type, tVec=res_df$r, yVec=res_df$border)
  #check that the FPCA object is valid
  fdapace::CheckData(fd_obj$Ly, fd_obj$Lt)
  #run the computation of the FPCA - would work with sparse data.
  fpca_obj <- fdapace::FPCA(fd_obj$Ly, fd_obj$Lt, list(plot = TRUE, kernel='rect'))
  #fdapace::CreatePathPlot(fpca_obj,K = 3, pch = 4,showObs = FALSE, showMean = TRUE)
  return(fpca_obj)
}
celltype_ls_new <- c("Ependymal", "OD Mature", "Microglia", "Ambiguous", "Astrocyte", "Endothelial", "Excitatory", "Inhibitory", "OD Immature", "Pericytes")
print(plotScaledMetric(celltype_ls_new, ppls, 'Lscaled', 'iso')$p)
res_df <- plotScaledMetric(celltype_ls_new, ppls, 'Lscaled', 'iso')$res_df

fpca_obj <- functional.pca.pp(res_df) 

fpca_pc_df <- as.data.frame(fpca_obj$xiEst) %>% rename(PC1 = V1, PC2 = V2)
fpca_pc_df$type <- celltype_ls_new
# assume they are in same order

ggplot(fpca_pc_df, aes(x=PC1, y=PC2, col = type, label = type)) +
  geom_text(hjust=0, vjust=0) +
  geom_point() +
  theme_light() +
  ggtitle("Biplot of the L curves of all Celltypes")
```

The interpretation of the locally-scaled $L$-function is similar to that of the inhomogeneous $L$-function. The correlation is strongest for Ependymal cells, followed by OD mature cells. Microglia cells are again close to the CSR Poisson process. Note that here, the curves of the Ependymal and OD mature cells stay always above the dashed Poisson line, unlike in the inhomogeneous version.

Given that our biological samples are both inhomogeneous and locally scaled by eye (can be tested as seen above), the locally scaled $L$-function seems a good variant for assessing correlation.

[MR: this last paragraph is a very important point; I wonder if we should say more. The problem is that there is a lot of build-up to this point .. and we should make it clear what to use in the general situation.]

#### Local Indicators of Spatial Association

It is worth noting that the $K$- and $L$-functions described above are summary statistics over the entire pattern (i.e., averaged over all points). However, if we know that there are different regions in our point pattern, an alternative strategy is to compute ``local'' contributions to these patterns, i.e., local $K$- ,$L$- or pair-correlation functions. Baddeley et. al. propose to compare these $n$ functions with so-called functional principal component analysis (see below). We will show here the example of the LISA version of the $L$-function [1].


##### Local $L$ function

```{r, error=FALSE, message=FALSE, warning=FALSE, results='hide'}
L_odmature_lisa <- localL(ppls$`OD Mature`)

df <- as.data.frame(L_odmature_lisa)
dfm <- reshape2::melt(df, "r")

get_sel <- dfm %>% filter(r > 200.1358 & r < 200.1360, variable != "theo") %>%
  mutate(sel = value) %>% select(variable, sel)

dfm <- dfm %>% left_join(get_sel)

p <- ggplot(dfm, aes(x=r, y=value, group=variable, colour=sel)) +
  geom_line() + 
  scale_color_continuous(type = "viridis") +
  geom_vline(xintercept = 200) +
  theme(legend.position = "none") +
  theme_light()

ppdf <- as.data.frame(pp) %>% filter(marks=="OD Mature")
ppdf$sel <- get_sel$sel # assume they are in same order

q <- ggplot(ppdf, aes(x=x, y=y, colour=sel)) + 
  geom_point() +
  scale_color_continuous(type = "viridis") +
  theme(legend.position = "none") +
  theme_light()
```


```{r, fig.height=5, fig.width=10}
p|q
```

In the case of the OD mature cells, we obtain further information with this plot. We note that there are two distinct populations of curves: those that are clearly above the CSR Poisson line [MR: do we know that the lower population is around CSR? Should we plot Poisson directly?] and others that are around/underneath the CSR line. This indicates that there are two different kinds of interactions in the OD mature cells. Stronger clustering (the upper part of the plot) and more random parts (lower part).

There are inhomogeneous versions of these (e.g. `localLinhom`) that are not shown here for brevity.

##### Functional PCA for the $n$ Curves

We apply functional PCA to retrieve the main trends in these individual curves. The idea of functional PCA is the same as for ordinary PCA but applied to functional data (i.e., each observation is a function instead of a point). For the $n$ functions above, functional PCA will recover the main trends in the data. 

```{r, error=FALSE, message=FALSE, warning=FALSE, results='hide'}
#adapted from the fdapace vignette
functional.pca.pp <- function(df){
  # df_fdob <- asinh(df %>% as.matrix / 50)  # [MR: should we transform? if so, how?]
  df_fdob <- df %>% as.matrix # [MR: should we transform? if so, how?]
  #remove theo column - we want only the actual estimations in there without the Poisson line theo
  if('r' %in% colnames(df) || 'theo' %in% colnames(df))
     df_fdob <- df_fdob[,!colnames(df_fdob) %in% c("r", "theo")]
  if('Ependymal' %in% colnames(df))
    df_fdob <- df_fdob[,!colnames(df_fdob) %in% c("trans",'iso')]

  #number of columns
  N <- ncol(df_fdob)
  #number of rows
  M <- nrow(df_fdob)
  #the x values at which all the curves were evaluated, here called tVec
  s <- df$r
  #create the FPCA object
  fd_obj <- fdapace::MakeFPCAInputs(IDs = rep(1:N, each=M),
                                    tVec=rep(s,N), yVec=df_fdob)
  print(which( unlist( lapply(fd_obj$Lt, 
                              function(x) length(x) != length(unique(x))))))
  #check that the FPCA object is valid
  fdapace::CheckData(fd_obj$Ly, fd_obj$Lt)
  #run the computation of the FPCA - would work with sparse data.
  fpca_obj <- fdapace::FPCA(fd_obj$Ly, fd_obj$Lt, 
                            list(plot = TRUE, dataType='Dense', kernel='rect'))
  fdapace::CreatePathPlot(fpca_obj,K = 3, pch = 4,
                          showObs = FALSE, showMean = TRUE)
  return(fpca_obj)
}

fpca_obj <- functional.pca.pp(L_odmature_lisa)
fpca_pc_df <- as.data.frame(fpca_obj$xiEst) %>% 
  rename(PC1 = V1, PC2 = V2, PC3 = V3)
fpca_pc_df$sel <- get_sel$sel # assume they are in same order

ggplot(fpca_pc_df, aes(x=PC1, y=PC2, col = sel)) + 
  scale_color_continuous(type = "viridis") +
  geom_point() +
  theme_light() +
  ggtitle("Biplot of the LISA L curves of the OD mature cells")

ggplot(ppdf, aes(x=x, y=y, colour = fpca_pc_df$PC1)) + 
  scale_color_continuous(type = "viridis") +
  geom_point()

ggplot(ppdf, aes(x=x, y=y, colour = fpca_pc_df$PC2)) + 
  scale_color_continuous(type = "viridis") +
  geom_point()

```

[MR: made a note in the code above. Should we transform before PCA?]

Here, we see the functional PCA for the OD mature cells. The Design plot tells us that we have a very dense dataset over the entire support [MR: do we need the 'Design plot'? it doesn't show much]. The mean curve displays the mean trend over all $n$ LISA $L$-curves (which is itself similar to the locally-scaled $L$-function). The scree plot indicates that the first eigenfunction explains more than $80 \%$ of the variance. The eigenfunction curves in the bottom right panel indicate the deviation from the mean curve.

Looking at the second plot, we see the smoothed mean curve and the individual curves that are reconstructed from the first three eigenfunctions. The first eigenfunction from the bottom right panel, $\phi_1$, is above the mean curve, which relates to the population of curves above the mean. $\phi_2$ is first above the mean curve and then lower than the mean curve. These curves are visible as well. Lastly, $\phi_3$ is curves that start low and pick up to be larger than the mean curve in the end. This is visible in e.g. the orange dashed line [2].

The last plot shows the biplot of the functional PCA [MR: should we mention "loadings"?]. Each point is a cell from the OD mature cells with the first two principal components plotted. The points are coloured as they were in the plots of the LISA $L$-curves. The first principal component clearly separates the two populations. [MR: here I also wonder whether plotting the points by their spatial location and colouring by their loadings of PC1-PC3 would be interesting to look at. I added an example in the code; to me, it looks interesting]

### Third-Order Summary Statistics

So far we have considered first- and second-order summary statistics and local (or inhomogeneous) adaptations of them. In the following, we will continue towards even higher-order statistics. In the second order, one considers (counts of) pairs (e.g., $K$ function). In a third-order setting, we would count triplets of points. A triplet is counted as the normalised expected value of triangles where all edges are smaller than the radius r [1].

$$
T(r) = \frac{1}{\lambda^3}\mathbb{E}\left[\sum_{i=1}^n\sum_{j=1\\j\neq i}^nm(x_i,x_j,u) | u \in X\right]
$$

here m is the maximum side of the triangle

$$
m(a,b,c) = \max(||a-b||,||a-c||,||b-c||)
$$

```{r}
p <- plotScaledMetric(celltype_ls, ppls, 'Tstat', 'trans')
p
```

[MR: should we skip the plot in this section?]

## Spacing

So far, most approaches considered intensity and correlation as measures to assess a point pattern. Next, we will look at measures of spacing and shortest-distances to assess spatial arrangements [1].

Baddeley et.al. summarises three basic distances to measure:

-   pairwise distance: $d_{i,j} = ||x_i-x_j||$
-   NN distances: $d_i = \min_{j \neq i}d_{ij}$
-   empty-space distance: $d(u) = \min_j||u-x_j||$

Note also that there are tests of CSR that are based on spacing, including the Clark-Evans and Hopkins-Skellam Index tests that were discussed above. [MR: maybe add ref to that section?]

````{=html}
<!-- ### Exploratory Graphics

#### Stienen Diagram

The stienen diagram is obtained by drawing circles around each point in the process of the size of its NN distance

```{r}
stienen(ppls$Ependymal)
stienen(ppls$`OD Mature`)
stienen(ppls$Microglia)
```

#### Dirichlet Tiles

Dirichlet tiles are considered as the space "that is closer to $x_i$ than to any other point in the pattern $x$:"

$$
C(x_i|x) = \{u \in \mathbb{R}^2:||u-x_i|| = \min_{j} ||u-x_j||\}
$$

These dirichlet tiles are convex polygons

```{r}
plot(dirichlet(ppls$Ependymal))
plot(dirichlet(ppls$`OD Mature`))
plot(dirichlet(ppls$Microglia))
``` 
-->
````

### Nearest Neighbour approaches

Nearest neighbour (NN) methods are based on the notion of "nearness". In particular, we introduce `nndist` from `spatstat`, a method to calculate the distances until $k$ NN are found. This function returns a curve for each specified $k$ [MR: do we need to say that a curve here is a density of a distribution?] for the $k$ neighbour distances. We can for instance collapse the $k$ curves into a mean curve per point pattern. This information of the mean nearest neighbour distance (MMND) can be summarised as a density [1].

```{r, cache=FALSE}
nndistance <- function(pp, nk){
  xy <- cbind(pp$x, pp$y)
  nndistances_k15 <- nndist(xy, k = nk) 
  nndistances_mean <- rowMeans(nndistances_k15)
  return(nndistances_mean)
}

#PRE: list of point pattern, corresponding celltypes of interest, functions to evaluate
#POST: result of the metric
metricRes_nndist <- function(ppls, celltype, fun){
  metric.res <- list(res = do.call(fun, args = list(pp=ppls[[celltype]], nk = seq(1:15))))
  metric.res$type = celltype
  return(metric.res)
}
# [MR: again, this function looks again like those before and maybe could be done as an all-in-one wrapper.]

#go through all defined celltypes and calculate the nearest-neighbour distance
res_ls <- lapply(celltype_ls, metricRes_nndist, fun = nndistance, ppls = ppls)
#initialise a dataframe for the metric values and the type information
res_df <- data.frame(metric = numeric(0), type = character(0))
# Loop through the res_ls list and combine the metric values with their corresponding type - ChatGPT
for (i in 1:length(res_ls)) {
  metric_values <- res_ls[[i]]$res
  metric_type <- rep(res_ls[[i]]$type, length(metric_values))
  df <- data.frame(metric = metric_values, type = metric_type)
  res_df <- rbind(res_df, df)
}
#plot the densities
p <- ggplot(res_df, aes(x=metric, col= type))+
    geom_density(linewidth=1)+
    scale_x_sqrt() +
    theme_light() +
    ggtitle('Sqrt of the Mean Nearest-Neighbour Distance')
p
```

In the MNND empirical distribution, the ependymal cells show the shortest NN distances, a reflection of their clustering. The OD mature cells have larger NN distances as well as a bimodal distribution, indicating a mix of longer and wider distances (as visible in the LISA $L$-functions). Microglia cells show the widest distances and the symmetry of the curve indicates similar distances throughout the field of view.

#### DBScan

Often, we are interested which spatial structures build a spatial unit. One way to answer this question is to use spatially aware clustering. 
Here we show one very basic approach, DBScan (density-based spatial clustering with applications for noise). DBScan is an algorithm that uses a parameter called minimal Features. If the number of points per cluster is smaller than the minimal number of features, it is either noise or it is merged with another cluster. Another parameter $\epsilon$ defines the distance at which a point has to lay in order to be part of a cluster. The value of this parameter $\epsilon$ can be determined by an elbow plot, similar to clustering resolution parameters in other algorithms.
[MR: can we make this reference below a proper reference?]
[Source](https://pro.arcgis.com/en/pro-app/latest/tool-reference/spatial-statistics/how-density-based-clustering-works.htm)

```{r, cache=FALSE}
pp_df <- as.data.frame(ppls$`OD Mature`)
#determine the correct epsilon neighbourhood
dbscan::kNNdistplot(pp_df, k=5)
```

Given the kNN distance plot we visually detect the "knee" of the curve to be at an distance $\epsilon$ of $200$. This value is needed for the computation of DBScan.

```{r}
#perform DBScan
pp_dbscan <- dbscan::dbscan(pp_df[-3], eps =200, minPts = 5)
plot(pp_df[-3], col = pp_dbscan$cluster)
```

We see that DBScan identifies four clusters. A top cluster in red and two main side clusters in black and green. There is another cluster in blue that is at the top right corner.

another lead to follow in detected spatial clusters is scan statistics with the \texttt{R} package \texttt{rflexscan}. [MR: unfinished?]

[MR: personally, I don't see a lot of value in the DBScan analysis, but you can try and convince me.]

### Nearest-neighbour function $G$ and empty-space function $F$

#### Definitions of $F$ and $G$ function

Under a stationary spatial point process, the empty-space distance is defined as:

$$
d(u,X) = \min\{||u-x_i||: x_i \in X\}
$$

[MR: what does one do for a non-stationary process?]

The empty space function is then the cumulative distribution function of the empty-space distances defined above:

$$
F(r) = \mathbb{P}\{d(u,X)\leq r\}.
$$

The NN distance is defined as:

$$
d_i = \min_{j\neq i}||x_j-x_i||.
$$

The NN distance distribution function $G(r)$ is then defined as:

$$
G(r) = \mathbb{P}\{d(x,X\backslash u \leq r |X\ has\ a\ point\ at\ u\}.
$$

For a homogeneous Poisson process, the NN distance distribution is identical to the empty-space function of the same process:

$$
G_{pois} \equiv F_{pois}.
$$

For a general point process, the $F$ and $G$ functions are different [1].

[MR: so actually, from the notation, I don't see why F and G would be different for a general point process .. they are simply two ways to express the distance to the nearest point. What am I missing?]

### Empty-space hazard

The $F$ and $G$ functions are, like the $K$ function, cumulative. The same disadvantages as with the $K$ function occur here too [MR: what disadvantages? maybe say them explicitly here]. Therefore, an analogue to the pair-correlation function would make sense to consider. For practical reasons, this is no longer the derivative of the $F$ function but rather a hazard rate [1]:

$$
h(r) = \frac{f(r)}{1-F(r)}.
$$

For a CSR process, the hazard rate is:

$$
h_{pois}(r) = 2 \pi \lambda r
$$
(i.e., linear in $r$).

Here we use a variance stabilising transformation as suggested by Baddeley et. al. This transformation means that if the process is completely spatial random, the hazard is equal to the intensity $\lambda$ [1].
 <!-- 
```{r}
fhazard_ependymal <- envelope(ppls$Ependymal, Fhazard, nsim=39, fix.n=TRUE,
transform=expression(./(2*pi*r)))
fhazard_ependymal$type <- 'Ependymal'

fhazard_odmature <- envelope(ppls$`OD Mature`, Fhazard, nsim=39, fix.n=TRUE,
transform=expression(./(2*pi*r)))
fhazard_odmature$type <- 'OD Mature'

fhazard_microglia <- envelope(ppls$Microglia, Fhazard, nsim=39, fix.n=TRUE,
transform=expression(./(2*pi*r)))
fhazard_microglia$type <- 'Microglia'

#create a list that combines bove values
fhazard_list<- rbind(fhazard_ependymal, fhazard_odmature, fhazard_microglia)

p <- ggplot(fhazard_list, aes(x=r, y=obs, col= type))+
  geom_line(key_glyph = "point")+
  guides(color = guide_legend(override.aes = list(size = 3))) +
  geom_ribbon(aes(ymin = lo, ymax = hi), alpha = 0.25, show.legend = FALSE)+
  ggtitle('Empty-space hazard')+
  geom_line(aes(x=r,y=theo),linetype = "dashed", show.legend = FALSE)+
  theme_light() #+ theme(legend.key.size = unit(0.5, "lines"))
p
```
 -->

### $J$-Function

The concepts of the empty-space function $F$ and the NN function $G$ are somewhat complementary. If one decreases, the other increases. A comparison of these two functions as a measure of CSR is the Hopkins-Skellam test (implemented above) [1].

Thus, a related approach is the $J$ function:

$$
J(r) = \frac{1-G(r)}{1-F(r)}.
$$

For a CSR process, $J_{pois} \equiv 1$, whereas values of $J(r) > 1$ are consistent with a regular (e.g., repelling) pattern, and \$J(r) \< 1 represents a clustered process [1].

```{r, fig.height = 10, fig.width = 7}
### need to redefine the plotting function, because bootstrap is not available for spacing functions ###
#PRE: Celltypes of interest, function to analyse, edge correction to perform
#POST: plot of the metric
plotSpacingMetric <- function(celltype_ls, ppls, fun, x,  edgecorr){
  res_df <- metricResToDF(celltype_ls, ppls, fun)
  #plot the curve
  p <- ggplot(res_df, aes(x=res_df[[x]], y=res_df[[edgecorr]], col= type))+
    geom_line()+
    ylab(edgecorr) +
    xlab(x) +
    ggtitle(paste0(fun, '-function'))+
    geom_line(aes(x=res_df[[x]], y=theo), linetype = 'dashed')+
    theme_light()
  return(p)
}
## [MR: same comment as above: can we have a super-wrapper?]

p_G <- plotSpacingMetric(celltype_ls, ppls, 'Gest', 'r', 'rs')

p_F <- plotSpacingMetric(celltype_ls, ppls, 'Fest', 'r', 'rs')

p_J <- plotSpacingMetric(celltype_ls, ppls, 'Jest', 'r', 'rs')
```

```{r, warning=FALSE, fig.height = 10, fig.width = 7}
p_G/p_F/p_J
```

### Accounting for Inhomogeneity in Spacing Functions

[MR: so if there are inhomogeneous versions, should we skip the plots above? Or not, because the inhomogeneous versions are inaccurate because they still assume correlation stationarity. What do we do?!? We need to be careful not to paint overselves into a corner.]

There are inhomogeneous variants of the spacing functions explained above

```{r}
p_G <- plotSpacingMetric(celltype_ls, ppls, 'Ginhom', 'r', 'bord')

p_F <- plotSpacingMetric(celltype_ls, ppls, 'Finhom', 'r', 'bord')

p_J <- plotSpacingMetric(celltype_ls, ppls, 'Jinhom', 'r', 'bord')
```

```{r, cache=FALSE, fig.height = 10, fig.width = 7}
p_G/p_F/p_J
```

Again here, comparing the homogeneous versions of the functions with the inhomogeneous ones reveals, we seem to solve one problem (inhomogeneity) by assuming correlation stationarity. As this is not a given, the inhomogeneous versions do not seem to be accurate. In fact, the homogeneous versions are more easily interpretable than the inhomogeneous alternatives.

### Nearest-Neighbour Orientation

Next to the NN distance, we can estimate the *orientation* of the neighbours, which gives an indication of the orientation of the spacing.

[MR: here just noting that we give no definition of the orientation metric, which counters the quite verbose definitions that we have seen up to this point.]

```{r}
p <- plotSpacingMetric(celltype_ls, ppls, 'nnorient', 'phi', 'bordm')
p
```

The values of $\phi$ correspond to the orientation of the point pattern. The horizontal axis goes from $180$ to $0$ (left to right) and the vertical from $90$ to $270$ (top to bottom) [MR: I find this description quite confusing, because you are not talking about the horizontal/vertical axis of the metric plot, but rather the orientations in the point patterns. And since certain $\phi$s represent these orientations, is it worth adding some shading of the x-axis to highlight this? Thicker lines would also benefit the old people reading this. And I wonder if faceting on celltype would help also. What does the Y-axis represent? And are there local variants of these? Maybe the clustered OD cells have a different orientation than the non-clustered?].
We can infer that the orientation of the Ependymal NNs is primarily along the vertical axis, OD mature cells do not show a clear orientation and microglial cells a horizontal orientation in their NNs with a modest peak at $\sim 180$ (orientation to the top).

Note also tha the concepts of spacing are not only usable in *point* pattern analysis but also more broadly in other spatial contexts (e.g., spacing between shapes instead of points).

### Edge Corrections

The same consideration about edge effects as for the $K$ (and related) functions need to be made for the spacing functions; uncorrected estimates are negatively biased as estimators for the real spacing functions [MR: what is meant by "real spacing functions" here?]. The easiest approach is to draw an artificial border and consider NNs within it. Other approaches are based on sampling. Yet another approach relates to survival analysis, with the idea is that a circle of a point to grows homogeneously with increasing radius until it hits the frame of the window and "dies". This gives survival distributions similar to censored data, where the Kaplan-Meier estimator is the optimal choice [1].

# Continuous Marks

## Setup

```{r, message=FALSE}
#| label: load-data 2

#subset the data to only look at sample ID 0.01
sub_2CT = spe[, spe$sample_id == "0.01" & spe$cluster_id %in% c("Astrocyte", "Inhibitory")]
(pp_2CT = .ppp(sub_2CT, marks = "cluster_id"))
#[MR: this above never gets used?]
```


In `spatstat`, a `mark` can basically take any value, discrete (as we have seen above) or  continuous (e.g., gene expression). In our example, we take the gene expression of some marker genes from Fig. 6 of the original publication.

```{r}
#  Genes from Fig. 6 of Moffitt et al. (2018)
genes <- c('Slc18a2', 'Esr1', 'Pgr')
gex <- assay(sub)[genes,] %>% t %>% as.matrix %>% 
  data.frame %>% set_rownames(NULL)

# gene expression to marks
marks(pp) <- gex
```

> TODO: better plotting?

```{r}
plot(pp)
```

[MR: what is the point of this plot?]

A pairs plot indicates spatial inhomogeneity, i.e. a spatial trend of the of the marks. This plot is only reasonable for a small number of marks however.

```{r}
pairs(as.data.frame(pp), panel = panel.smooth, pch=".")
```

[MR: also not sure the point of this plot?]

The `Smooth` command uses cross-validation to select the smoothing bandwidth of the Gaussian kernel. This estimated kernel can be used for visual inspection of the dataset.

```{r}
ppsmooth <- Smooth(pp, bw.smoothppp)
plot(ppsmooth)
```
From the estimated intensity plot, we can see that the expression of the marker genes is clearly inhomogeneous. [MR: what does it mean that marker gene expression is inhomogeneous .. I thought inhomogeneity had to do with the location of points?!]

NN interpolations uses the nearest mark to measure the intensity at each spatial location. This is conceptually similar to taking a very small bandwith for the Gaussian kernel. [MR: wouldn't a small bandwidth use also the mark at the spatial location too, in addition to it's neighbours?]

```{r}
plot(nnmark(pp))
```
We can use the average value of neighbouring points to predict the expression of a gene at each point. We can then plot the actual marks versus the fitted values to detect anomalies. For example, `Esr1` shows a clear half moon shape in the middle of the image, where the actual values are much higher than the fitted. This gives further indication of structure in the gene expression.[MR: I don't really understand the interpretation here.]

```{r}
mfit <- Smooth(pp, bw.smoothppp, at="points")
res <- marks(pp) - mfit

plot(setmarks(pp, res))
```

[MR: I don't really have a good intuition of what these plots mean, but I think the plots are anyways hard to interpret. The scale is a bit weird .. I think the point size has some meaning, but that's really hard to see. Colours would be better, but I'm not even sure these plots tell us much interesting. Also, since we are taking differences between values and a smooth fit, the scale of the data becomes important; are we on a "good" scale?]

## Summary functions for continuous marks

As in the discrete case, summary functions assume that the point process is stationary.

### Mark correlation function

The mark correlation function measures the dependence between two marks for two points at distance $r$. It is not a correlation in the classical sense, since it cannot take non-negative values [MR: was this a typo? is this correct?]. Instead, a value of 1 indicates no correlation between the marks [MR: does this mean that negative correlation is not allowed? Or, would it appear as 'less than 1'?]. The generalized mark correlation function is given by:

$$ k_f(r) = \frac{\mathbb{E}[f(m(u),m(v))]}{\mathbb{E}[f(M,M')]},$$
where $f(m_1,m_2)$ is a test function with two arguments (representing the two marks) and returns a non-negative value [MR: what are $u$ and $v$ in the formula above?]. For continuous non-negative marks, the canonical choice for $f$ is typically  $f(m_1,m_2)= m_1 m_2$. $M$ and $M$ represent independent, identically distributed random points with same distribution as the mark of a randomly chosen point. This denominator is chosen such that random marks have a mark correlation of 1 [1].

```{r}
plot(markcorr(pp))
```

[MR: maybe we should create our own plot here? do we need both 'iso' and 'trans' .. is there any info that is represented in the scale of the y-axis? Put all 3 genes on one plot?]

We can compare the mark correlation function to a pointwise simulation envelope in which we generate 5 simulations of random labeling.  
```{r}
ppEsr1 <- subset(pp, select = 'Esr1')
markcorr.Esr1 <- envelope(ppEsr1, markcorr, nsim=10)

plot(markcorr.Esr1)
```

The plot indicates that the mark correlation function is significantly different from the random case. The positive association of expression of the Esr1 gene declines with distance. This is consistent with clustering we saw in the residual plot above. 

<!-- We can calculate the cross correlation function between the different gene expression measurments, the diagonal is equivalent to `markcorr` -->

<!-- ```{r} -->
<!-- mc <- markcrosscorr(pp) -->
<!-- ``` -->


<!-- ```{r} -->
<!-- par(mar = c(1, 1, 1, 1)) -->
<!-- plot(mc) -->
<!-- ``` -->

### Mark-weighted $K$-function

The mark-weigthed $K$-function is a generalization of the $K$-function in which the contribution from each pair of points is weighted by a function of their respective marks. The mark-weighted $K$-function is given by:

$$K_f(r) = \frac 1  \lambda \frac{C_f(r)}{E[ f(M_1, M_2) ]},$$
where:

$$ C_f(r) = E \left[ \sum_{x \in X} f(m(u), m(x)) 1\{0 < ||u - x|| \le r\} \;  \big| \; u \in X \right], $$ 

is equivalent to the unnormalized mark-weighted $K$-function. For every point $u$, we sum the euclidean distance $||u - x||$ of all other points $x$ that are within a distance $r$. This sum is weighted by the function $f(.,.)$ of the marks of $u$ and $x$. The function is standardized by the expected value of $f(M, M)$ where $M, M$ represent independent [MR: note mix of M1, M2 .. and M, M'], identically distributed random points with the same distribution as the mark of a randomly chosen point [1].

In the scenario of random labeling [MR: what does random labeling mean?], the mark-weighted $K$-function corresponds to Ripley's $K$-function.    

Also here, the canonical function is: $f(m_1, m_2) = m_1 m_2$. 

```{r}
ppEsr1 <- subset(pp, select = 'Esr1')

K.Esr1L <- Kmark(ppEsr1, function(m1,m2) {m1*m2})
plot(K.Esr1L)
```
It is important to note that theoretical value is not very informative since it represents the $K$-function of a Poisson point process and the underlying point process might not be Poisson. Therefore we compare the mark-weighted with its unmarked analogue. 

Here we will compare the $L$-functions, which are variance-stabilized $K$-functions.

```{r}
L.Esr1L <- Kmark(ppEsr1, function(m1,m2) {m1*m2}, returnL = TRUE)
Lest.ppEsr1 <- Lest(ppEsr1, nlarge=7000)
plot(eval.fv(L.Esr1L - Lest.ppEsr1))
```

As for other functions, we can calculate a simulation envelope for the mark-weighted K-function to get confidence intervals [1].

```{r}
plot(envelope(ppEsr1, fun=Kmark, returnL=TRUE, nsim=50))
```

[MR: all these plots need an interpretation, I think.]

[MR: do we need a summary section, just to remind the reader all the ground we covered?]

# Appendix

## Sources

[1] Baddeley, A., Rubak, E., & Turner, R. (2015). Spatial point patterns: methodology and applications with R. CRC press.

[2] Ramsay JO, Silverman BW (2005). Functional Data Analysis. 2nd edition. Springer-Verlag,
New York.

## Session info

```{r, cache=FALSE}
#| label: session-info
sessionInfo()
```
